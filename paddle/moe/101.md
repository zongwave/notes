# Mixture of Experts (MoE) 机制

## 1. MoE 介绍

MoE 是一种稀疏激活的模型架构，通过动态选择少量专家（experts）来处理每个 token，从而在保持模型性能的同时显著降低计算成本。

## 2. MoE 机制的核心原理

MoE 的核心思想是稀疏激活：对于每个 token，只激活少量专家（例如 Top-2），而不是所有专家，从而减少计算量。

### 2.1 MoE 的基本结构

MoE 层通常由以下核心组件组成：

#### 专家（Experts）

一组小型神经网络（通常是 FFN），每个专家负责处理特定的输入模式。

#### 门控（Gate）

一个路由机制，为每个 token 选择 Top-K 专家。

#### 组合（Combine）

将选中的专家输出加权组合，生成最终输出。

### 2.2 MoE 的工作流程

#### 输入

接收输入 hidden_states（形状为 [batch_size, seq_len, hidden_size]）。

#### 门控

通过门控网络（Gate）计算每个 token 对每个专家的得分（scores），选择 Top-K 专家。

#### 分发

将 token 分发给选中的专家。

#### 专家计算

每个专家处理分配的 token，生成输出。

#### 组合

将专家输出加权组合，生成最终输出。

### 2.3 MoE 的优势

#### 计算效率

通过稀疏激活，计算量显著减少。例如，若有 8 个专家但只激活 2 个，计算量减少到 25%。

#### 模型容量

增加专家数量可以提升模型容量，而不显著增加推理时间。

#### 分布式支持

专家可以分配到不同设备，实现专家并行（expert parallelism）。
